{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbSKS8GfdO0ZBEHpaQW6ax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/S48avio/GOOGLE-ADK/blob/main/Weather_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwLjtarvX1Wz",
        "outputId": "a00edd5a-1e49-42d1-be0b-e294199264b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/10.5 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m9.6/10.5 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/278.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstallation complete.\n"
          ]
        }
      ],
      "source": [
        "# @title Step 0: Setup and Installation\n",
        "# Install ADK and LiteLLM for multi-model support\n",
        "\n",
        "!pip install google-adk -q\n",
        "!pip install litellm -q\n",
        "\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erqgfFnjYLNC",
        "outputId": "9586abc8-f12d-43ef-f5a6-dd0b89253a9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configure API Keys (Replace with your actual keys!)\n",
        "\n",
        "# --- IMPORTANT: Replace placeholders with your real API keys ---\n",
        "\n",
        "# Gemini API Key (Get from Google AI Studio: https://aistudio.google.com/app/apikey)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDk2WIsmksssNsBejjjBHdE2YnRYbj-4Rlb3w\" # <--- REPLACE\n",
        "\n",
        "\n",
        "\n",
        "# --- Verify Keys (Optional Check) ---\n",
        "print(\"API Keys Set:\")\n",
        "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
        "\n",
        "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
        "\n",
        "\n",
        "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCBdurxOYZdn",
        "outputId": "51374bb1-fa9a-4003-9d56-ce03a422c69e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys Set:\n",
            "Google API Key set: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Model Constants for easier use ---\n",
        "\n",
        "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
        "\n",
        "\n",
        "print(\"\\nEnvironment configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXVarzhpYs-t",
        "outputId": "a70479d0-bd3b-4a06-b658-8bb7bc861389"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Environment configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My weather agent that uses Open-Meteo Api for weather information"
      ],
      "metadata": {
        "id": "ftHJxXSdy4nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# --- API Endpoints ---\n",
        "# Step 1: Geocoding API (converts city name to coordinates)\n",
        "GEOCODING_URL = \"https://geocoding-api.open-meteo.com/v1/search\"\n",
        "# Step 2: Forecast API (gets weather data using coordinates)\n",
        "FORECAST_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "def get_weather(city: str) -> dict:\n",
        "    \"\"\"Retrieves the hourly temperature forecast for a specified city for 1 day.\n",
        "\n",
        "    This function first converts the city name to coordinates (Geocoding) and\n",
        "    then fetches the hourly temperature data from the Open-Meteo API.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the weather information or an error message.\n",
        "    \"\"\"\n",
        "    print(f\"--- Tool: get_weather called for city: {city} ---\")\n",
        "\n",
        "    try:\n",
        "        # 1. Geocoding: Convert City Name to Coordinates\n",
        "        geo_params = {'name': city, 'count': 1, 'language': 'en', 'format': 'json'}\n",
        "        geo_response = requests.get(GEOCODING_URL, params=geo_params)\n",
        "        geo_response.raise_for_status()\n",
        "        geo_data = geo_response.json()\n",
        "\n",
        "        if not geo_data.get('results'):\n",
        "            return {\"status\": \"error\", \"error_message\": f\"Sorry, could not find coordinates for '{city}'.\"}\n",
        "\n",
        "        # Use the first result (usually the most popular city)\n",
        "        location = geo_data['results'][0]\n",
        "        latitude = location['latitude']\n",
        "        longitude = location['longitude']\n",
        "\n",
        "        found_city = location['name']\n",
        "        country = location['country']\n",
        "\n",
        "        # 2. Forecast: Get Hourly Temperature Data\n",
        "        forecast_params = {\n",
        "            'latitude': latitude,\n",
        "            'longitude': longitude,\n",
        "            'hourly': 'temperature_2m',\n",
        "            'forecast_days': 1\n",
        "        }\n",
        "        forecast_response = requests.get(FORECAST_URL, params=forecast_params)\n",
        "        forecast_response.raise_for_status()\n",
        "        forecast_data = forecast_response.json()\n",
        "\n",
        "        # 3. Process and Compile Report\n",
        "        times = forecast_data['hourly']['time']\n",
        "        temperatures = forecast_data['hourly']['temperature_2m']\n",
        "\n",
        "        # Find the min/max for the 24-hour period\n",
        "        min_temp = min(temperatures)\n",
        "        max_temp = max(temperatures)\n",
        "\n",
        "        # Create a detailed report (you can customize the output format)\n",
        "        report = (\n",
        "            f\"Hourly Temperature Forecast for {found_city}, {country} \"\n",
        "            f\"(Lat: {latitude:.2f}, Lon: {longitude:.2f}) for the next 24 hours:\\n\"\n",
        "            f\"Min Temperature: {min_temp}°C\\n\"\n",
        "            f\"Max Temperature: {max_temp}°C\\n\"\n",
        "            f\"Hourly Data (Time: Temp):\\n\"\n",
        "        )\n",
        "        # Add a few key hourly points for brevity\n",
        "        for i in range(0, len(times), 4): # Sample every 4 hours\n",
        "             report += f\"  {times[i].split('T')[1].split(':')[0]}h: {temperatures[i]}°C\\n\"\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"city\": found_city,\n",
        "            \"report\": report,\n",
        "            \"min_temperature_celsius\": min_temp,\n",
        "            \"max_temperature_celsius\": max_temp,\n",
        "            \"hourly_data\": list(zip(times, temperatures)) # Provides all raw data\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"HTTP Error: Could not fetch data. ({e})\"}\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        return {\"status\": \"error\", \"error_message\": \"Connection Error: Could not connect to the API server.\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"error_message\": f\"An unexpected error occurred: {e}\"}\n",
        "\n",
        "# Example tool usage\n",
        "print(get_weather(\"London\"))\n",
        "print(get_weather(\"Kochi\"))\n",
        "print(get_weather(\"Nowhereville\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYevsyW6zFf5",
        "outputId": "4eae131f-38f3-4957-f496-ee8aa02e1106"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: London ---\n",
            "{'status': 'success', 'city': 'London', 'report': 'Hourly Temperature Forecast for London, United Kingdom (Lat: 51.51, Lon: -0.13) for the next 24 hours:\\nMin Temperature: 4.1°C\\nMax Temperature: 8.8°C\\nHourly Data (Time: Temp):\\n  00h: 7.1°C\\n  04h: 5.8°C\\n  08h: 5.1°C\\n  12h: 7.8°C\\n  16h: 8.3°C\\n  20h: 5.7°C\\n', 'min_temperature_celsius': 4.1, 'max_temperature_celsius': 8.8, 'hourly_data': [('2025-11-17T00:00', 7.1), ('2025-11-17T01:00', 6.4), ('2025-11-17T02:00', 5.8), ('2025-11-17T03:00', 5.9), ('2025-11-17T04:00', 5.8), ('2025-11-17T05:00', 5.7), ('2025-11-17T06:00', 5.5), ('2025-11-17T07:00', 5.3), ('2025-11-17T08:00', 5.1), ('2025-11-17T09:00', 5.4), ('2025-11-17T10:00', 6.1), ('2025-11-17T11:00', 7.0), ('2025-11-17T12:00', 7.8), ('2025-11-17T13:00', 8.6), ('2025-11-17T14:00', 8.8), ('2025-11-17T15:00', 8.7), ('2025-11-17T16:00', 8.3), ('2025-11-17T17:00', 7.6), ('2025-11-17T18:00', 6.9), ('2025-11-17T19:00', 6.3), ('2025-11-17T20:00', 5.7), ('2025-11-17T21:00', 5.2), ('2025-11-17T22:00', 4.7), ('2025-11-17T23:00', 4.1)]}\n",
            "--- Tool: get_weather called for city: Kochi ---\n",
            "{'status': 'success', 'city': 'Kochi', 'report': 'Hourly Temperature Forecast for Kochi, Japan (Lat: 33.55, Lon: 133.53) for the next 24 hours:\\nMin Temperature: 11.1°C\\nMax Temperature: 22.4°C\\nHourly Data (Time: Temp):\\n  00h: 17.2°C\\n  04h: 22.4°C\\n  08h: 19.0°C\\n  12h: 15.6°C\\n  16h: 13.1°C\\n  20h: 11.8°C\\n', 'min_temperature_celsius': 11.1, 'max_temperature_celsius': 22.4, 'hourly_data': [('2025-11-17T00:00', 17.2), ('2025-11-17T01:00', 19.4), ('2025-11-17T02:00', 20.7), ('2025-11-17T03:00', 21.8), ('2025-11-17T04:00', 22.4), ('2025-11-17T05:00', 22.4), ('2025-11-17T06:00', 21.5), ('2025-11-17T07:00', 20.0), ('2025-11-17T08:00', 19.0), ('2025-11-17T09:00', 17.8), ('2025-11-17T10:00', 16.8), ('2025-11-17T11:00', 16.3), ('2025-11-17T12:00', 15.6), ('2025-11-17T13:00', 14.9), ('2025-11-17T14:00', 14.2), ('2025-11-17T15:00', 13.6), ('2025-11-17T16:00', 13.1), ('2025-11-17T17:00', 12.8), ('2025-11-17T18:00', 12.4), ('2025-11-17T19:00', 12.1), ('2025-11-17T20:00', 11.8), ('2025-11-17T21:00', 11.4), ('2025-11-17T22:00', 11.1), ('2025-11-17T23:00', 11.4)]}\n",
            "--- Tool: get_weather called for city: Nowhereville ---\n",
            "{'status': 'error', 'error_message': \"Sorry, could not find coordinates for 'Nowhereville'.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define the Weather Agent\n",
        "# Use one of the model constants defined earlier\n",
        "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"weather_agent\",\n",
        "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
        "    description=\"Provides weather information for specific cities.\",\n",
        "    instruction=\"You are a helpful weather assistant. \"\n",
        "                \"When the user asks for the weather in a specific city, \"\n",
        "                \"use the 'get_weather' tool to find the information. \"\n",
        "                \"If the tool returns an error, inform the user politely. \"\n",
        "                \"If the tool is successful, present the weather report clearly.\",\n",
        "    tools=[get_weather], # Pass the function directly\n",
        ")\n",
        "\n",
        "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIow8Xgh2CsL",
        "outputId": "29af8ed7-7bdd-4a75-fd86-ea1dd60022eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 'weather_agent' created using model 'gemini-2.0-flash'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Session Service and Runner\n",
        "\n",
        "# --- Session Management ---\n",
        "# Key Concept: SessionService stores conversation history & state.\n",
        "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Define constants for identifying the interaction context\n",
        "APP_NAME = \"weather_tutorial_app\"\n",
        "USER_ID = \"user_1\"\n",
        "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
        "\n",
        "# Create the specific session where the conversation will happen\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME,\n",
        "    user_id=USER_ID,\n",
        "    session_id=SESSION_ID\n",
        ")\n",
        "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "\n",
        "# --- Runner ---\n",
        "# Key Concept: Runner orchestrates the agent execution loop.\n",
        "runner = Runner(\n",
        "    agent=weather_agent, # The agent we want to run\n",
        "    app_name=APP_NAME,   # Associates runs with our app\n",
        "    session_service=session_service # Uses our session manager\n",
        ")\n",
        "print(f\"Runner created for agent '{runner.agent.name}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hUhWUUZ2Soo",
        "outputId": "7b1209b7-244d-4cf2-ecb6-07c7223abf4e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
            "Runner created for agent 'weather_agent'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define Agent Interaction Function\n",
        "\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "async def call_agent_async(query: str, runner, user_id, session_id):\n",
        "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
        "  print(f\"\\n>>> User Query: {query}\")\n",
        "\n",
        "  # Prepare the user's message in ADK format\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "\n",
        "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
        "\n",
        "  # Key Concept: run_async executes the agent logic and yields Events.\n",
        "  # We iterate through events to find the final answer.\n",
        "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "      # You can uncomment the line below to see *all* events during execution\n",
        "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
        "\n",
        "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
        "      if event.is_final_response():\n",
        "          if event.content and event.content.parts:\n",
        "             # Assuming text response in the first part\n",
        "             final_response_text = event.content.parts[0].text\n",
        "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
        "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
        "          # Add more checks here if needed (e.g., specific error codes)\n",
        "          break # Stop processing events once the final response is found\n",
        "\n",
        "  print(f\"<<< Agent Response: {final_response_text}\")"
      ],
      "metadata": {
        "id": "h1_vd4qb2XSL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run the Initial Conversation\n",
        "\n",
        "# We need an async function to await our interaction helper\n",
        "async def run_conversation():\n",
        "    await call_agent_async(\"What is the weather like in London?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "    await call_agent_async(\"How about Paris?\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
        "\n",
        "    await call_agent_async(\"Tell me the weather in New York\",\n",
        "                                       runner=runner,\n",
        "                                       user_id=USER_ID,\n",
        "                                       session_id=SESSION_ID)\n",
        "\n",
        "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
        "await run_conversation()\n",
        "\n",
        "# --- OR ---\n",
        "\n",
        "# Uncomment the following lines if running as a standard Python script (.py file):\n",
        "# import asyncio\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         asyncio.run(run_conversation())\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x6hm-502g9h",
        "outputId": "9d152dc7-6e6f-4b9a-c084-b337838f6f7b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> User Query: What is the weather like in London?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: London ---\n",
            "<<< Agent Response: Here is the weather forecast for London: The temperature will range from a minimum of 4.1°C to a maximum of 8.8°C.\n",
            "\n",
            ">>> User Query: How about Paris?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: Paris ---\n",
            "<<< Agent Response: The weather in Paris will range from a minimum of 4.6°C to a maximum of 9.4°C.\n",
            "\n",
            ">>> User Query: Tell me the weather in New York\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tool: get_weather called for city: New York ---\n",
            "<<< Agent Response: The weather in New York will range from a minimum of 1.2°C to a maximum of 5.9°C.\n"
          ]
        }
      ]
    }
  ]
}